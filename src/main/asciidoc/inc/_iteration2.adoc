[[iteration-2]]
= Iteration II

The Iteration II will <<itr2-deprecated-modules,deprecate>> few of the https://netflix.github.io/[NetFlix OSS] components that are superflous
inside https://kubernetes.io[Kubernetes] or https://www.openshift.com[OpenShift]. The following sections shows how to get
the Iteration II deployed on to https://kubernetes.io[Kubernetes] or https://www.openshift.com[OpenShift]. This iteration
uses the https://github.com/spring-cloud-incubator/spring-cloud-kubernetes[Spring Cloud Kubernetes] -
the http://projects.spring.io/spring-cloud/[Spring Cloud] based discovery client for Kubernetes

[[itr2-java-msa-platform-circa]]
image::./iteration-2_java_msa_circa.png[title=Java Microservices,align=center]

The picture above shows how <<itr1-java-msa-platform-circa,Java Microservices Platform - Iteration-1>> has evolved post deployment to https://www.openshift.com[OpensShift], especially how components like Eureka and Config Server became superflous and has been <<itr2-deprecated-modules,deprecated>>.  

[[itr2-application-setup]]
== Setup

[[itr2-clone-source]]
=== Clone

[source,sh]
----
git clone -b iteration-2 https://github.com/redhat-developer-demos/brewery.git
----

[NOTE]
====
Through out this document we will call the directory where the project was cloned as _$PROJECT_HOME_
====


[[itr2-pre-req]]
== Pre-Requisite

[[itr2-pre-req-general]]
=== General
The spring-cloud-kubernetes library used in the project requires the `default` service account to have view permissions, to enable that we execute the following command,

[source,sh]
----
oc policy add-role-to-user view -z default -n $(oc project -q)
----

[TIP]
====
The Service Account `default` does not have any permission, in order to allow `default` SA to lookup the ConfigMaps within the namespace, it need to be added with `view`
role 
====

[[itr2-pre-req-rabbitmq]]
=== RabbitMQ 

The https://hub.docker.com/_/rabbitmq/[RabbimtMQ] container image when run is run using `root` user (UID `0`).  The OpenShift by default does not allow container 
images to be run with UID `0`. In order to allow that we need to define polices and attach to Kubernetes Service Account, to have better clarity and control 
we create a separate service account called **brewery** and add the required https://docs.openshift.org/latest/admin_guide/manage_scc.html[Security Context Constraints(SCC)] 
to allow running the container as UID `0` user.  The following commands adds the required SCC to the service account **brewery**,

[source,sh]
----
oc adm policy add-scc-to-user privileged -z brewery <1>

oc adm policy add-scc-to-user anyuid -z brewery  <1>
----

<1> **brewery** service is created when RabbitMQ is deployed

[[itr2-deployable-apps]]
== Deploy Applications

.Application List
[cols="1*^,1,1,5"]
|===
| |Application| Folder | Remarks

|icon:check[color: green]
|<<itr2-rabbitmq>>
|*$PROJECT_HOME*/extras/rabbitmq
|Message Broker - https://www.rabbitmq.com/

|icon:check[color: green]
|common
|*$PROJECT_HOME*/common
|Common shared library

|icon:check[color: green]
|common-zipkin-stream
|*$PROJECT_HOME*/ommon-zipkin-stream
|Common shared library for the projects that uses the Sleuth Zipkin Stream for tracing

|[red]#*X*#
|[red]#eureka#
|[red]#*$PROJECT_HOME*/eureka#
|Application will use https://kubernetes.io/docs/concepts/services-networking/service/[Kubernetes Services]

|[red]#*X*#
|[red]#config-server#
|[red]#*$PROJECT_HOME*/config-server#
|Application will use https://kubernetes.io/docs/tasks/configure-pod-container/configmap/[Kubernetes ConfigMaps]

|icon:check[color: green]
|<<itr2-zipkin-server>>
|*$PROJECT_HOME*/zipkin-server
| http://zipkin.io/[Distributed Tracing system]

|icon:check[color: green]
|<<itr2-zuul>>
|*$PROJECT_HOME*/zuul
| https://github.com/Netflix/zuul/wiki[Java based Proxy]

|icon:check[color: green]
|<<itr2-ingredients>>
|*$PROJECT_HOME*/ingredients
|

|icon:check[color: green]
|<<itr2-reporting>>
|*$PROJECT_HOME*/reporting
|

|icon:check[color: green]
|<<itr2-brewing>>
|*$PROJECT_HOME*/brewing
|

|icon:check[color: green]
|<<itr2-presenting>>
|*$PROJECT_HOME*/presenting
|

|===

[[itr2-build-app]]
=== Building

The Iteration II of the brewery application has migrated all the projects to http://maven.apache.org/[Apache Maven] based build.

To build the application run the following command:

[source,sh]
----
./mvnw -N install <1>
./mvnw clean install <2>
----
<1> This will install the brewery parent pom in local maven repository
<2> This will build the applications, if the minishift or OpenShift cluster is running, this will trigger `s2i` builds
of the respective application as well

[[itr2-deploy-to-openshift]]
=== Deploying to OpenShift

The following section details on how to deploy the Iteration II to OpenShift.

[IMPORTANT]
====
Ensure that all <<itr2-pre-req,Pre-Requisite>> are done before starting deployment.
====

[[itr2-rabbitmq]]
==== RabbitMQ

[[itr2-rabbitmq-local]]
===== Local Deployment

Go to the directory  *$PROJECT_HOME/extras/rabbitmq*, and execute the following command

[source,sh]
----
./mvnw -Dfabric8.mode=kubernetes clean fabric8:deploy
----

[[itr2-rabbitmq-cloud]]
===== External Cloud Deployment

Sometimes you might have access to docker socket typical case when deploying to external cloud, in those cases you can run the following set of commands,

[source,sh]
----
./mvnw clean fabric8:resource
oc apply -f target/classes/META-INF/fabric8/openshift.yml
----

This will take some time to get it running as the deployment needs to download the `rabbitmq` docker image

[[itr2-zipkin-server]]
==== Zipkin Server

Go to the directory  *$PROJECT_HOME/zipkin-server*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-zuul]]
==== Zuul

Go to the directory  *$PROJECT_HOME/zuul*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-ingredients]]
==== Ingredients

Go to the directory  *$PROJECT_HOME/ingredients*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-reporting]]
==== Reporting

Go to the directory  *$PROJECT_HOME/reporting*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-brewing]]
==== Brewing

Go to the directory  *$PROJECT_HOME/brewing*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-presenting]]
==== Presenting

Go to the directory  *$PROJECT_HOME/presenting*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-acceptance-testing]]
== Acceptance Testing

The *$PROJECT_HOME/acceptance-tests* holds the test cases for testing the application.  To perform
we need to have have some ports forwarded from Kubernetes/OpenShift to localhost(where you build the application)

[source,sh]
----
oc port-forward zipkin-1-06wmt 9411:8080 <1>
oc port-forward presenting-1-wzhfn 9991:8080 <2>
----

<1> forward port 8080 from Zipkin pod to listen on localhost:9411
<2> forward port 8080 from Presenting pod to listen on localhost:9991

NOTE: Please update the pod names based on your local deployment

To run acceptance testing, execute following command from $PROJECT_HOME,

[source,sh]
----
 ./mvnw clean test
----

[[itr2-deprecated-modules]]
== Deprecated Modules

As part of Iteration-II the following modules have been deprecated,

* Eureka
* Config Server
* common-zipkin
* common-zipkin-old
* zookeeper
* docker
